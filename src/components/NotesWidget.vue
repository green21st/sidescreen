<template>
  <div class="widget notes-widget" @dblclick="handleWidgetDoubleClick">
    <h2 v-if="!showGame" class="widget-header">ç¬”è®°</h2>
    <div class="widget-content">
      <div v-if="!showGame" class="notes-content">
        <div class="notes-form">
          <div class="input-container">
            <textarea 
              v-model="newNote" 
              placeholder="å†™ç‚¹ä»€ä¹ˆ..." 
              class="notes-input"
              ref="notesTextarea"
              @dblclick.stop
            ></textarea>
            <button 
              @click="toggleSpeechRecognition" 
              class="speech-btn"
              :class="{ 
                'recording': isRecording, 
                'disabled': !speechRecognitionAvailable && !useLocalRecording && !useSpeechService
              }"
              :title="getSpeechButtonTitle()"
            >
              <span v-if="isRecording" class="recording-indicator"></span>
              ğŸ¤
            </button>
          </div>
          <div class="form-actions">
            <div class="speech-status" v-if="isRecording">
              æ­£åœ¨å½•éŸ³... <span class="recording-time">{{ recordingTime }}s</span>
            </div>
            <div class="speech-error" v-if="recognitionError && !isRecording">
              {{ getErrorMessage(recognitionError) }}
            </div>
            <div class="action-buttons">
              <button @click="addNote" class="notes-add-btn primary-btn">ä¿å­˜</button>
            </div>
          </div>
        </div>
        
        <div class="notes-list" v-if="notes.length > 0">
          <div 
            v-for="note in notes" 
            :key="note.id" 
            class="note-item fade-in"
            @dblclick.stop
          >
            <div class="note-content">{{ note.content }}</div>
            <div class="note-footer">
              <div class="note-timestamp">{{ formatDate(note.timestamp) }}</div>
              <button @click="removeNote(note.id)" class="note-delete-btn danger-btn">
                åˆ é™¤
              </button>
            </div>
          </div>
        </div>
        
        <div v-else class="notes-empty">
          <div class="empty-icon">ğŸ“‹</div>
          <div>æ²¡æœ‰ç¬”è®°</div>
        </div>
      </div>
      
      <!-- ä¿„ç½—æ–¯æ–¹å—æ¸¸æˆç»„ä»¶ -->
      <div v-if="showGame" class="game-container">
        <TetrisGame 
          :isVisible="showGame" 
          @close="closeGame" 
        />
      </div>
    </div>
    
    <!-- è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨æ—¶çš„æç¤º -->
    <div v-if="showSpeechUnavailableModal" class="speech-unavailable-modal">
      <div class="modal-content">
        <h3>è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨</h3>
        <p>{{ speechUnavailableReason }}</p>
        <div class="modal-actions">
          <button @click="showSpeechUnavailableModal = false" class="primary-btn">ç¡®å®š</button>
        </div>
      </div>
    </div>
    
    <!-- å½•éŸ³å®Œæˆåçš„æç¤º -->
    <div v-if="showRecordingModal" class="speech-unavailable-modal">
      <div class="modal-content">
        <h3>å½•éŸ³å®Œæˆ</h3>
        <p>ç”±äºè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨ï¼Œæ‚¨å¯ä»¥ï¼š</p>
        <div class="recording-actions">
          <button @click="playRecording" class="secondary-btn">
            <span>ğŸ”Š</span> æ’­æ”¾å½•éŸ³
          </button>
          <button @click="saveRecordingToNote" class="primary-btn">
            æ‰‹åŠ¨è¾“å…¥æ–‡å­—
          </button>
        </div>
        <div class="modal-actions">
          <button @click="showRecordingModal = false" class="danger-btn">å…³é—­</button>
        </div>
      </div>
    </div>
  </div>
</template>

<script setup lang="ts">
import { ref, onMounted, onUnmounted, computed, watch } from 'vue';
// å¯¼å…¥è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡
import * as iflytekSpeech from '../services/iflytekSpeechService';
// å¯¼å…¥æœ¬åœ°è¯­éŸ³è¾“å…¥æœåŠ¡
import * as localSpeech from '../services/localSpeechService';
// å¯¼å…¥ä¿„ç½—æ–¯æ–¹å—æ¸¸æˆç»„ä»¶
import TetrisGame from './games/TetrisGame.vue';

// è¯­éŸ³è¯†åˆ«æ¥å£å£°æ˜
declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
    electronAPI?: {
      requestMicrophonePermission: () => Promise<boolean>;
      checkMicrophonePermission: () => Promise<boolean>;
      testSpeechRecognition: () => Promise<{ available: boolean; message?: string; reason?: string }>;
      logDebug: (message: string) => void;
    };
  }
}

interface Note {
  id: number;
  content: string;
  timestamp: Date;
}

interface SpeechConfig {
  appId: string;
  apiKey: string;
  apiSecret: string;
  enabled: boolean;
}

const props = defineProps<{
  notes: Note[];
  speechConfig: SpeechConfig;
}>();

const emit = defineEmits<{
  (e: 'add-note', content: string): void;
  (e: 'remove-note', id: number): void;
}>();

const newNote = ref('');
const notesTextarea = ref<HTMLTextAreaElement | null>(null);
const isRecording = ref(false);
const recordingTime = ref(0);
const recordingTimer = ref<number | null>(null);
const recognitionError = ref<string | null>(null);
const isNetworkAvailable = ref(navigator.onLine);
const speechRecognitionAvailable = ref(true);
const speechUnavailableReason = ref('');
const showSpeechUnavailableModal = ref(false);
const speechRecognitionTested = ref(false);
const useLocalRecording = ref(false);
const recordingBlob = ref<Blob | null>(null);
const showRecordingModal = ref(false);

// è¯­éŸ³è¯†åˆ«ç›¸å…³
let recognition: SpeechRecognition | null = null;
let recognitionTimeout: number | null = null;
let speechRecognitionInstance: { stop: () => void } | null = null;
let useSpeechService = false; // æ˜¯å¦ä½¿ç”¨è¯­éŸ³è¯†åˆ«æœåŠ¡

// æ¸¸æˆç›¸å…³çŠ¶æ€
const showGame = ref(false);

// æ£€æŸ¥æ˜¯å¦åœ¨ Electron ç¯å¢ƒä¸­
const isElectron = () => {
  return window && (window.process?.type || window.electronAPI);
};

// è®°å½•è°ƒè¯•ä¿¡æ¯
const logDebug = (message: string) => {
  console.log(`[NotesWidget] ${message}`);
  if (window.electronAPI?.logDebug) {
    window.electronAPI.logDebug(`[NotesWidget] ${message}`);
  }
};

// è·å–è¯­éŸ³æŒ‰é’®æç¤ºæ–‡æœ¬
const getSpeechButtonTitle = () => {
  if (!speechRecognitionAvailable.value && !useLocalRecording.value && !useSpeechService) {
    return 'è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨';
  }
  return isRecording.value ? 'åœæ­¢å½•éŸ³' : 'è¯­éŸ³è¾“å…¥';
};

// è·å–é”™è¯¯æ¶ˆæ¯
const getErrorMessage = (error: string) => {
  switch (error) {
    case 'network':
      return 'ç½‘ç»œé”™è¯¯ï¼Œè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨';
    case 'not-allowed':
      return 'éº¦å…‹é£è®¿é—®è¢«æ‹’ç»';
    case 'audio-capture':
      return 'æ— æ³•æ•è·éŸ³é¢‘ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£è®¾å¤‡';
    case 'aborted':
      return 'è¯­éŸ³è¯†åˆ«å·²ä¸­æ­¢';
    case 'no-speech':
      return 'æœªæ£€æµ‹åˆ°è¯­éŸ³';
    default:
      return `è¯­éŸ³è¯†åˆ«é”™è¯¯: ${error}`;
  }
};

// æµ‹è¯•è¯­éŸ³è¯†åˆ«å¯ç”¨æ€§
const testSpeechRecognition = async () => {
  if (speechRecognitionTested.value) {
    return;
  }
  
  logDebug('Testing speech recognition availability');
  speechRecognitionTested.value = true;
  
  // æ£€æŸ¥æ˜¯å¦æ”¯æŒæœ¬åœ°å½•éŸ³
  const localRecordingSupported = localSpeech.checkMicrophonePermission();
  logDebug(`Local recording supported: ${localRecordingSupported}`);
  
  // å¦‚æœæ”¯æŒæœ¬åœ°å½•éŸ³ï¼Œå³ä½¿è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨éº¦å…‹é£æŒ‰é’®
  if (await localRecordingSupported) {
    useLocalRecording.value = true;
  }
  
  // æ£€æŸ¥ç½‘ç»œè¿æ¥
  if (!navigator.onLine) {
    logDebug('Network is offline, speech recognition unavailable');
    speechRecognitionAvailable.value = false;
    speechUnavailableReason.value = 'ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œè¯­éŸ³è¯†åˆ«éœ€è¦ç½‘ç»œè¿æ¥';
    return;
  }
  
  // åœ¨ Electron ç¯å¢ƒä¸­ä½¿ç”¨ä¸»è¿›ç¨‹æµ‹è¯•
  if (isElectron() && window.electronAPI?.testSpeechRecognition) {
    try {
      const result = await window.electronAPI.testSpeechRecognition();
      logDebug(`Speech recognition test result: ${JSON.stringify(result)}`);
      
      if (!result.available) {
        speechRecognitionAvailable.value = false;
        speechUnavailableReason.value = result.message || 'è¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨';
        
        if (result.reason === 'google_blocked') {
          logDebug('Google services blocked, speech recognition unavailable');
          speechUnavailableReason.value = 'åœ¨ä¸­å›½å¤§é™†åœ°åŒºï¼Œè°·æ­Œè¯­éŸ³è¯†åˆ«æœåŠ¡å¯èƒ½æ— æ³•è®¿é—®ã€‚è¯·ä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡ã€‚';
        }
      }
    } catch (error) {
      logDebug(`Speech recognition test error: ${error}`);
      speechRecognitionAvailable.value = true; // å‡è®¾å¯ç”¨ï¼Œé¿å…è¯¯æŠ¥
    }
  } else {
    // åœ¨æµè§ˆå™¨ç¯å¢ƒä¸­å°è¯•åˆ›å»ºå®ä¾‹æµ‹è¯•
    try {
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      if (!SpeechRecognition) {
        logDebug('SpeechRecognition API not supported');
        speechRecognitionAvailable.value = false;
        speechUnavailableReason.value = 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½';
        return;
      }
      
      // å°è¯•åˆ›å»ºå®ä¾‹
      const testRecognition = new SpeechRecognition();
      testRecognition.lang = 'zh-CN';
      
      // è®¾ç½®è¶…æ—¶
      const timeout = setTimeout(() => {
        logDebug('Speech recognition test timed out');
        speechRecognitionAvailable.value = true; // å‡è®¾å¯ç”¨ï¼Œé¿å…è¯¯æŠ¥
      }, 3000);
      
      // è®¾ç½®é”™è¯¯å¤„ç†
      testRecognition.onerror = (event) => {
        clearTimeout(timeout);
        logDebug(`Speech recognition test error: ${event.error}`);
        
        if (event.error === 'network') {
          speechRecognitionAvailable.value = false;
          speechUnavailableReason.value = 'ç½‘ç»œé”™è¯¯ï¼Œè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨ã€‚åœ¨ä¸­å›½å¤§é™†åœ°åŒºï¼Œæ­¤åŠŸèƒ½å¯èƒ½å—é™ã€‚è¯·ä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡ã€‚';
        } else if (event.error === 'not-allowed') {
          speechRecognitionAvailable.value = false;
          speechUnavailableReason.value = 'éº¦å…‹é£è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®';
        } else {
          // å…¶ä»–é”™è¯¯æš‚æ—¶è®¤ä¸ºå¯ç”¨ï¼Œé¿å…è¯¯æŠ¥
          speechRecognitionAvailable.value = true;
        }
      };
      
      // è®¾ç½®ç»“æŸå¤„ç†
      testRecognition.onend = () => {
        clearTimeout(timeout);
        logDebug('Speech recognition test ended');
      };
      
      // å°è¯•å¯åŠ¨
      try {
        testRecognition.start();
        setTimeout(() => {
          try {
            testRecognition.stop();
          } catch (e) {
            // å¿½ç•¥åœæ­¢é”™è¯¯
          }
        }, 1000);
      } catch (error) {
        clearTimeout(timeout);
        logDebug(`Error starting test recognition: ${error}`);
        // å¯åŠ¨å¤±è´¥ï¼Œä½†ä»ç„¶å‡è®¾å¯ç”¨ï¼Œé¿å…è¯¯æŠ¥
        speechRecognitionAvailable.value = true;
      }
    } catch (error) {
      logDebug(`Error in speech recognition test: ${error}`);
      speechRecognitionAvailable.value = true; // å‡è®¾å¯ç”¨ï¼Œé¿å…è¯¯æŠ¥
    }
  }
};

// ä½¿ç”¨ Electron API è¯·æ±‚éº¦å…‹é£æƒé™
const requestElectronMicrophonePermission = async () => {
  logDebug('Requesting Electron microphone permission');
  if (window.electronAPI && window.electronAPI.requestMicrophonePermission) {
    try {
      const result = await window.electronAPI.requestMicrophonePermission();
      logDebug(`Electron microphone permission result: ${JSON.stringify(result)}`);
      return result.granted;
    } catch (error) {
      logDebug(`Electron microphone permission request failed: ${error}`);
      return false;
    }
  }
  return false;
};

// æ£€æŸ¥ Electron éº¦å…‹é£æƒé™
const checkElectronMicrophonePermission = async () => {
  logDebug('Checking Electron microphone permission');
  if (window.electronAPI && window.electronAPI.checkMicrophonePermission) {
    try {
      const result = await window.electronAPI.checkMicrophonePermission();
      logDebug(`Electron microphone permission status: ${JSON.stringify(result)}`);
      return result.status === 'granted';
    } catch (error) {
      logDebug(`Electron microphone permission check failed: ${error}`);
      return false;
    }
  }
  return false;
};

// åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
const initSpeechRecognition = () => {
  try {
    logDebug('Initializing speech recognition');
    
    // æ£€æŸ¥æµè§ˆå™¨æ˜¯å¦æ”¯æŒè¯­éŸ³è¯†åˆ«
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
    if (!SpeechRecognition) {
      logDebug('SpeechRecognition API not supported');
      return false;
    }
    
    logDebug('Creating SpeechRecognition instance');
    recognition = new SpeechRecognition();
    
    // é…ç½®è¯­éŸ³è¯†åˆ«
    recognition.continuous = true;
    recognition.interimResults = true;
    recognition.lang = 'zh-CN'; // è®¾ç½®è¯­è¨€ä¸ºä¸­æ–‡
    recognition.maxAlternatives = 1;
    
    // å¤„ç†è¯†åˆ«ç»“æœ
    recognition.onresult = (event) => {
      logDebug(`Recognition result received: ${event.results.length} results`);
      let interimTranscript = '';
      let finalTranscript = '';
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript;
        logDebug(`Transcript ${i}: ${transcript} (confidence: ${event.results[i][0].confidence})`);
        
        if (event.results[i].isFinal) {
          finalTranscript += transcript;
        } else {
          interimTranscript += transcript;
        }
      }
      
      // å°†è¯†åˆ«ç»“æœæ·»åŠ åˆ°æ–‡æœ¬æ¡†
      if (finalTranscript) {
        logDebug(`Adding final transcript to note: ${finalTranscript}`);
        newNote.value += finalTranscript;
      }
    };
    
    // å¤„ç†é”™è¯¯
    recognition.onerror = (event) => {
      logDebug(`Speech recognition error: ${event.error}`);
      recognitionError.value = event.error;
      
      if (event.error === 'not-allowed') {
        alert('éº¦å…‹é£è®¿é—®è¢«æ‹’ç»ï¼Œè¯·æ£€æŸ¥æƒé™è®¾ç½®');
      } else if (event.error === 'no-speech') {
        logDebug('No speech detected');
        // æ— è¯­éŸ³è¾“å…¥ï¼Œå¯èƒ½æ˜¯éº¦å…‹é£æ²¡æœ‰æ¥æ”¶åˆ°å£°éŸ³
        // ä¸éœ€è¦åœæ­¢å½•éŸ³ï¼Œç»§ç»­ç­‰å¾…
        return;
      } else if (event.error === 'audio-capture') {
        alert('æ— æ³•æ•è·éŸ³é¢‘ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£è®¾å¤‡');
      } else if (event.error === 'network') {
        // ç½‘ç»œé”™è¯¯å¤„ç†æ”¹è¿›
        logDebug('Network error detected, speech recognition service unavailable');
        
        // æ˜¾ç¤ºé”™è¯¯æç¤ºæ¨¡æ€æ¡†
        speechUnavailableReason.value = 'ç½‘ç»œé”™è¯¯ï¼Œè¯­éŸ³è¯†åˆ«æœåŠ¡ä¸å¯ç”¨ã€‚\nåœ¨ä¸­å›½å¤§é™†åœ°åŒºï¼Œæ­¤åŠŸèƒ½å¯èƒ½å—é™ã€‚';
        showSpeechUnavailableModal.value = true;
        
        // æ£€æŸ¥æ˜¯å¦åœ¨çº¿
        if (!navigator.onLine) {
          speechUnavailableReason.value = 'æ‚¨å½“å‰å¤„äºç¦»çº¿çŠ¶æ€ï¼Œè¯­éŸ³è¯†åˆ«éœ€è¦ç½‘ç»œè¿æ¥';
        }
        
        // å°è¯•åˆ‡æ¢åˆ°å¤‡ç”¨æœåŠ¡
        // if (!useSpeechService) {
        //   logDebug('Attempting to switch to iFlytek speech service');
        //   useSpeechService = true;
        //   stopRecording();
        //   setTimeout(() => {
        //     startRecording();
        //   }, 500);
        //   return;
        // }
      } else if (event.error === 'aborted') {
        logDebug('Speech recognition aborted');
        // ç”¨æˆ·æˆ–ç³»ç»Ÿä¸­æ­¢äº†è¯­éŸ³è¯†åˆ«ï¼Œä¸éœ€è¦æ˜¾ç¤ºé”™è¯¯
      } else {
        // æ˜¾ç¤ºé”™è¯¯æç¤ºæ¨¡æ€æ¡†
        speechUnavailableReason.value = `è¯­éŸ³è¯†åˆ«é”™è¯¯: ${event.error}`;
        showSpeechUnavailableModal.value = true;
      }
      
      stopRecording();
    };
    
    // å¤„ç†è¯†åˆ«å¼€å§‹
    recognition.onstart = () => {
      logDebug('Speech recognition started');
      isRecording.value = true;
      recognitionError.value = null;
    };
    
    // å¤„ç†è¯†åˆ«ç»“æŸ
    recognition.onend = () => {
      logDebug('Speech recognition ended');
      
      // å¦‚æœæ˜¯ä¸»åŠ¨åœæ­¢ï¼Œåˆ™ä¸é‡æ–°å¼€å§‹
      if (!isRecording.value || recognitionError.value) {
        stopRecording();
        return;
      }
      
      // è®¾ç½®ä¸€ä¸ªçŸ­æš‚çš„å»¶è¿Ÿï¼Œé¿å…ç«‹å³é‡å¯å¯èƒ½å¯¼è‡´çš„é—®é¢˜
      if (recognitionTimeout) {
        clearTimeout(recognitionTimeout);
      }
      
      recognitionTimeout = window.setTimeout(() => {
        try {
          if (isRecording.value && recognition) {
            recognition.start();
            logDebug('Recognition restarted');
          }
        } catch (error) {
          logDebug(`Error restarting recognition: ${error}`);
          stopRecording();
        }
      }, 300);
    };
    
    logDebug('Speech recognition initialized successfully');
    return true;
  } catch (error) {
    logDebug(`Error initializing speech recognition: ${error}`);
    return false;
  }
};

// å¼€å§‹æœ¬åœ°å½•éŸ³
const startLocalRecording = async () => {
  logDebug('Starting local recording');
  
  try {
    await localSpeech.startRecording({
      onStart: () => {
        isRecording.value = true;
        recordingTime.value = 0;
        
        // å¼€å§‹è®¡æ—¶
        if (recordingTimer.value) {
          clearInterval(recordingTimer.value);
        }
        
        recordingTimer.value = window.setInterval(() => {
          recordingTime.value++;
          
          // å¦‚æœå½•éŸ³æ—¶é—´è¶…è¿‡60ç§’ï¼Œè‡ªåŠ¨åœæ­¢
          if (recordingTime.value >= 60) {
            logDebug('Recording time limit reached (60s), stopping');
            stopLocalRecording();
          }
        }, 1000);
      },
      onStop: (state) => {
        isRecording.value = false;
        
        // æ¸…é™¤è®¡æ—¶å™¨
        if (recordingTimer.value) {
          clearInterval(recordingTimer.value);
          recordingTimer.value = null;
        }
        
        // ä¿å­˜å½•éŸ³ Blob
        if (state.audioBlob) {
          recordingBlob.value = state.audioBlob;
          showRecordingModal.value = true;
        }
      },
      onError: (error) => {
        logDebug(`Local recording error: ${error}`);
        recognitionError.value = 'local-recording-error';
        
        // æ˜¾ç¤ºé”™è¯¯æç¤º
        speechUnavailableReason.value = error;
        showSpeechUnavailableModal.value = true;
        
        stopLocalRecording();
      }
    });
  } catch (error) {
    logDebug(`Error starting local recording: ${error}`);
    recognitionError.value = 'local-recording-error';
    
    // æ˜¾ç¤ºé”™è¯¯æç¤º
    speechUnavailableReason.value = `å¯åŠ¨å½•éŸ³å¤±è´¥: ${error}`;
    showSpeechUnavailableModal.value = true;
  }
};

// åœæ­¢æœ¬åœ°å½•éŸ³
const stopLocalRecording = () => {
  logDebug('Stopping local recording');
  
  localSpeech.stopRecording();
  
  isRecording.value = false;
  
  // æ¸…é™¤è®¡æ—¶å™¨
  if (recordingTimer.value) {
    clearInterval(recordingTimer.value);
    recordingTimer.value = null;
  }
};

// æ’­æ”¾å½•éŸ³
const playRecording = () => {
  if (recordingBlob.value) {
    localSpeech.playRecording(recordingBlob.value);
  }
};

// ä¿å­˜å½•éŸ³
const saveRecordingToNote = () => {
  // è¿™é‡Œæˆ‘ä»¬åªæ˜¯æç¤ºç”¨æˆ·æ‰‹åŠ¨è¾“å…¥ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å®é™…çš„è¯­éŸ³è¯†åˆ«åŠŸèƒ½
  showRecordingModal.value = false;
};

// å¼€å§‹è®¯é£è¯­éŸ³è¯†åˆ«
const startSpeechRecognition = async () => {
  try {
    logDebug('Starting iFlytek speech recognition');
    isRecording.value = true;
    recordingTime.value = 0;
    recognitionError.value = null;
    
    // å¼€å§‹è®¡æ—¶
    if (recordingTimer.value) {
      clearInterval(recordingTimer.value);
    }
    
    recordingTimer.value = window.setInterval(() => {
      recordingTime.value++;
      
      // å¦‚æœå½•éŸ³æ—¶é—´è¶…è¿‡60ç§’ï¼Œè‡ªåŠ¨åœæ­¢
      if (recordingTime.value >= 60) {
        logDebug('Recording time limit reached (60s), stopping');
        stopRecording();
      }
    }, 1000);
    
    // å¼€å§‹è®¯é£è¯­éŸ³è¯†åˆ«
    speechRecognitionInstance = await iflytekSpeech.startRecognition({
      onStart: () => {
        logDebug('iFlytek speech recognition started');
      },
      onStop: () => {
        logDebug('iFlytek speech recognition stopped');
        stopRecording();
      },
      onResult: (text, isFinal) => {
        logDebug(`Received recognition result: ${text} (${isFinal ? 'final' : 'interim'})`);
        // ä¸ç®¡æ˜¯å¦æ˜¯æœ€ç»ˆç»“æœï¼Œéƒ½æ›´æ–°è¾“å…¥æ¡†
        const textarea = notesTextarea.value;
        if (textarea) {
          const start = textarea.selectionStart || 0;
          const end = textarea.selectionEnd || 0;
          const content = newNote.value;
          newNote.value = content.substring(0, start) + text + content.substring(end);
          // æ›´æ–°å…‰æ ‡ä½ç½®
          setTimeout(() => {
            textarea.selectionStart = textarea.selectionEnd = start + text.length;
            textarea.focus();
          }, 0);
        } else {
          // å¦‚æœæ— æ³•è·å–å…‰æ ‡ä½ç½®ï¼Œç›´æ¥è¿½åŠ åˆ°æœ«å°¾
          newNote.value += text;
        }
      },
      onError: (error) => {
        logDebug(`iFlytek speech recognition error: ${error}`);
        recognitionError.value = error;
        stopRecording();
      }
    });
  } catch (error) {
    logDebug(`Error in startSpeechRecognition: ${error}`);
    recognitionError.value = String(error);
    stopRecording();
  }
};

// å¼€å§‹å½•éŸ³
const startRecording = async () => {
  logDebug('Starting recording');
  
  // å¦‚æœä½¿ç”¨æœ¬åœ°å½•éŸ³
  if (useLocalRecording.value && !speechRecognitionAvailable.value) {
    startLocalRecording();
    return;
  }
  
  // å¦‚æœä½¿ç”¨è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡
  if (useSpeechService) {
    logDebug('Using iFlytek speech service');
    if (props.speechConfig.enabled && props.speechConfig.appId && props.speechConfig.apiKey) {
      await startSpeechRecognition();
      return;
    } else {
      logDebug('iFlytek speech service is not available');
      useSpeechService = false;
    }
  }
  
  // å¦‚æœè¯­éŸ³è¯†åˆ«ä¸å¯ç”¨ï¼Œæ˜¾ç¤ºæç¤º
  if (!speechRecognitionAvailable.value) {
    logDebug('Speech recognition not available');
    showSpeechUnavailableModal.value = true;
    return;
  }
  
  // æ£€æŸ¥ç½‘ç»œè¿æ¥
  if (!navigator.onLine) {
    speechUnavailableReason.value = 'æ‚¨å½“å‰å¤„äºç¦»çº¿çŠ¶æ€ï¼Œè¯­éŸ³è¯†åˆ«éœ€è¦ç½‘ç»œè¿æ¥';
    showSpeechUnavailableModal.value = true;
    logDebug('Cannot start recording: offline');
    return;
  }
  
  // ä½¿ç”¨ Web Speech API
  if (!recognition && !initSpeechRecognition()) {
    speechUnavailableReason.value = 'æ‚¨çš„æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«åŠŸèƒ½æˆ–éº¦å…‹é£è®¿é—®è¢«æ‹’ç»';
    showSpeechUnavailableModal.value = true;
    logDebug('Failed to initialize speech recognition');
    return;
  }
  
  try {
    // åœ¨ Electron ç¯å¢ƒä¸­ï¼Œå…ˆç¡®ä¿æœ‰éº¦å…‹é£æƒé™
    if (isElectron() && window.electronAPI) {
      const hasPermission = await requestElectronMicrophonePermission();
      if (!hasPermission) {
        recognitionError.value = 'not-allowed';
        logDebug('Microphone permission denied in Electron');
        return;
      }
    }
    
    // å¼€å§‹è¯­éŸ³è¯†åˆ«
    startSpeechRecognition();
  } catch (error) {
    logDebug(`Error starting recording: ${error}`);
    speechUnavailableReason.value = 'å¯åŠ¨è¯­éŸ³è¯†åˆ«å¤±è´¥ï¼Œè¯·æ£€æŸ¥éº¦å…‹é£æƒé™';
    showSpeechUnavailableModal.value = true;
  }
};

// åœæ­¢å½•éŸ³
const stopRecording = () => {
  logDebug('Stopping recording');
  
  // é˜²æ­¢é‡å¤æ‰§è¡Œ
  if (!isRecording.value) {
    return;
  }
  
  isRecording.value = false;
  
  // æ¸…é™¤è®¡æ—¶å™¨
  if (recordingTimer.value) {
    clearInterval(recordingTimer.value);
    recordingTimer.value = null;
  }
  
  // æ ¹æ®å½“å‰ä½¿ç”¨çš„æœåŠ¡æ¥åœæ­¢
  if (useSpeechService && speechRecognitionInstance) {
    try {
      speechRecognitionInstance.stop();
      speechRecognitionInstance = null;
    } catch (error) {
      logDebug(`Error stopping iFlytek speech recognition: ${error}`);
    }
  } else if (recognition) {
    try {
      recognition.stop();
    } catch (error) {
      logDebug(`Error stopping Web Speech API recording: ${error}`);
    }
  } else if (useLocalRecording.value) {
    stopLocalRecording();
  }
};

// åˆ‡æ¢è¯­éŸ³è¯†åˆ«çŠ¶æ€
const toggleSpeechRecognition = () => {
  // å¦‚æœæ—¢ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«ä¹Ÿä¸æ”¯æŒæœ¬åœ°å½•éŸ³
  if (!speechRecognitionAvailable.value && !useLocalRecording.value && !useSpeechService) {
    showSpeechUnavailableModal.value = true;
    return;
  }
  
  if (isRecording.value) {
    logDebug('Toggling speech recognition: stopping');
    stopRecording();
  } else {
    logDebug('Toggling speech recognition: starting');
    startRecording();
  }
};

const addNote = () => {
  if (newNote.value.trim()) {
    emit('add-note', newNote.value.trim());
    newNote.value = '';
    
    // å¦‚æœæ­£åœ¨å½•éŸ³ï¼Œåœæ­¢å½•éŸ³
    if (isRecording.value) {
      stopRecording();
    }
  }
};

const removeNote = (id: number) => {
  emit('remove-note', id);
};

const formatDate = (date: Date) => {
  return new Date(date).toLocaleString('zh-CN', {
    month: 'short',
    day: 'numeric',
    hour: '2-digit',
    minute: '2-digit'
  });
};

// ç›‘å¬è¯­éŸ³è¯†åˆ«é…ç½®å˜åŒ–
watch(() => props.speechConfig, async (newConfig) => {
  if (newConfig) {
    logDebug(`Speech config changed: enabled=${newConfig.enabled}`);
    
    // åˆå§‹åŒ–è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡
    if (newConfig.enabled && newConfig.appId && newConfig.apiKey) {
      try {
        const initialized = await iflytekSpeech.init({
          appId: newConfig.appId,
          apiKey: newConfig.apiKey,
          apiSecret: newConfig.apiSecret,
          enabled: newConfig.enabled
        });
        useSpeechService = initialized;
        logDebug(`iFlytek speech service initialized: ${initialized}`);
      } catch (error) {
        logDebug(`Error initializing iFlytek speech service: ${error}`);
        useSpeechService = false;
      }
    } else {
      useSpeechService = false;
    }
  }
}, { immediate: true, deep: true });

// åˆ‡æ¢æ¸¸æˆæ˜¾ç¤ºçŠ¶æ€
const toggleGame = () => {
  showGame.value = !showGame.value;
};

// å¤„ç†ç»„ä»¶åŒå‡»äº‹ä»¶
const handleWidgetDoubleClick = (event) => {
  // æ£€æŸ¥äº‹ä»¶æºæ˜¯å¦æ˜¯æ–‡æœ¬åŒºåŸŸæˆ–ç¬”è®°é¡¹
  const isTextarea = event.target.tagName === 'TEXTAREA';
  const isNoteItem = event.target.closest('.note-item');
  
  // å¦‚æœåŒå‡»çš„æ˜¯æ–‡æœ¬åŒºåŸŸæˆ–ç¬”è®°é¡¹ï¼Œä¸è§¦å‘æ¸¸æˆ
  if (isTextarea || isNoteItem) {
    return;
  }
  
  // åˆ‡æ¢æ¸¸æˆæ˜¾ç¤º
  toggleGame();
};

// å…³é—­æ¸¸æˆ
const closeGame = () => {
  showGame.value = false;
};

// ç»„ä»¶æŒ‚è½½æ—¶æ£€æŸ¥éº¦å…‹é£æƒé™
onMounted(async () => {
  logDebug('Component mounted');
  
  // è®¾ç½®ç½‘ç»œç›‘å¬
  setupNetworkListeners();
  
  // æµ‹è¯•è¯­éŸ³è¯†åˆ«å¯ç”¨æ€§
  await testSpeechRecognition();
  
  // åœ¨ Electron ç¯å¢ƒä¸­ï¼Œé¢„å…ˆæ£€æŸ¥éº¦å…‹é£æƒé™
  if (isElectron()) {
    logDebug('In Electron environment, checking microphone permission');
    
    if (window.electronAPI) {
      // ä½¿ç”¨ Electron API æ£€æŸ¥æƒé™
      const hasPermission = await checkElectronMicrophonePermission();
      logDebug(`Electron microphone permission check result: ${hasPermission}`);
    } else {
      // å›é€€åˆ°æ ‡å‡† Web API
      try {
        const permissionStatus = await navigator.permissions?.query({ name: 'microphone' as PermissionName });
        logDebug(`Web API microphone permission status: ${permissionStatus?.state}`);
      } catch (err) {
        logDebug(`Error querying microphone permission: ${err}`);
      }
    }
  }
  
  // é¢„åˆå§‹åŒ–è¯­éŸ³è¯†åˆ«
  try {
    const initialized = initSpeechRecognition();
    logDebug(`Speech recognition pre-initialization: ${initialized ? 'successful' : 'failed'}`);
  } catch (error) {
    logDebug(`Error pre-initializing speech recognition: ${error}`);
  }
});

// ç»„ä»¶å¸è½½æ—¶æ¸…ç†èµ„æº
onUnmounted(() => {
  logDebug('Component unmounting, cleaning up resources');
  
  if (isRecording.value) {
    stopRecording();
  }
  
  if (recordingTimer.value) {
    clearInterval(recordingTimer.value);
    recordingTimer.value = null;
  }
  
  if (recognitionTimeout) {
    clearTimeout(recognitionTimeout);
    recognitionTimeout = null;
  }
  
  // ç¡®ä¿é‡Šæ”¾è¯­éŸ³è¯†åˆ«èµ„æº
  if (recognition) {
    try {
      recognition.abort();
      logDebug('Recognition aborted');
    } catch (error) {
      logDebug(`Error aborting recognition: ${error}`);
    }
    recognition = null;
  }
  
  // ç§»é™¤ç½‘ç»œç›‘å¬å™¨
  window.removeEventListener('online', () => {});
  window.removeEventListener('offline', () => {});
});
</script>

<style scoped>
.notes-widget {
  display: flex;
  flex-direction: column;
  height: 100%;
  padding: 10px;
  min-height: 200px;
  position: relative;
  cursor: default;
}

.widget-header {
  margin-top: 0;
  margin-bottom: 10px;
  font-size: clamp(1rem, 1.5vw, 1.2rem);
  border-bottom: 1px solid var(--accent-color);
  padding-bottom: 5px;
  flex-shrink: 0;
}

.widget-content {
  display: flex;
  flex-direction: column;
  flex: 1;
  overflow: hidden;
}

.notes-content {
  display: flex;
  flex-direction: column;
  height: 100%;
  width: 100%;
}

.notes-form {
  display: flex;
  flex-direction: column;
  margin-bottom: 10px;
  flex-shrink: 0;
}

.input-container {
  position: relative;
  width: 100%;
  margin-bottom: 5px;
}

.notes-input {
  width: 100%;
  height: clamp(50px, 10vh, 80px);
  padding: 8px;
  padding-right: 40px; /* ä¸ºè¯­éŸ³æŒ‰é’®ç•™å‡ºç©ºé—´ */
  border: 1px solid rgba(255, 255, 255, 0.3);
  background-color: rgba(var(--widget-rgb), 0.3);
  color: var(--text-color);
  border-radius: 4px;
  resize: vertical;
  font-size: clamp(0.8rem, 1.2vw, 0.9rem);
  box-sizing: border-box;
}

.notes-input::placeholder {
  color: rgba(255, 255, 255, 0.6);
}

.speech-btn {
  position: absolute;
  right: 8px;
  top: 8px;
  width: 30px;
  height: 30px;
  border-radius: 50%;
  background-color: rgba(255, 255, 255, 0.1);
  border: none;
  color: var(--text-color);
  font-size: 16px;
  display: flex;
  align-items: center;
  justify-content: center;
  cursor: pointer;
  transition: all 0.2s ease;
}

.speech-btn:hover {
  background-color: rgba(255, 255, 255, 0.2);
}

.speech-btn.recording {
  background-color: rgba(231, 76, 60, 0.7);
  animation: pulse 1.5s infinite;
}

.recording-indicator {
  position: absolute;
  width: 8px;
  height: 8px;
  border-radius: 50%;
  background-color: #e74c3c;
  top: 0;
  right: 0;
}

.form-actions {
  display: flex;
  justify-content: space-between;
  align-items: center;
  margin-bottom: 5px;
}

.action-buttons {
  display: flex;
  align-items: center;
  gap: 10px;
  margin-left: auto;
}

.speech-status {
  font-size: 12px;
  color: var(--text-color);
  display: flex;
  align-items: center;
}

.recording-time {
  margin-left: 5px;
  font-weight: bold;
  color: #e74c3c;
}

.notes-add-btn {
  padding: 8px 15px;
  background-color: var(--accent-color);
  color: white;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  font-size: clamp(0.8rem, 1vw, 0.9rem);
}

.notes-list {
  flex: 1;
  overflow-y: auto;
  padding-right: 5px;
  min-height: 50px;
}

.note-item {
  padding: 12px;
  margin-bottom: 12px;
  background-color: rgba(255, 255, 255, 0.05);
  border-radius: 4px;
  transition: all 0.3s ease;
  border-left: 3px solid var(--accent-color);
  height: fit-content;
}

.note-item:hover {
  background-color: rgba(255, 255, 255, 0.1);
  transform: translateX(2px);
}

.note-content {
  margin-bottom: 10px;
  white-space: pre-wrap;
  line-height: 1.4;
  color: var(--text-color);
  font-size: clamp(0.8rem, 1.2vw, 0.9rem);
  word-break: break-word;
}

.note-footer {
  display: flex;
  justify-content: space-between;
  align-items: center;
  font-size: clamp(0.7rem, 1vw, 0.8rem);
  color: var(--text-color);
  opacity: 0.7;
  border-top: 1px solid rgba(255, 255, 255, 0.1);
  padding-top: 8px;
}

.note-delete-btn {
  background: none;
  border: none;
  color: #ff6b6b;
  cursor: pointer;
  font-size: clamp(0.7rem, 1vw, 0.8rem);
  padding: 4px 8px;
}

.notes-empty {
  flex: 1;
  display: flex;
  flex-direction: column;
  justify-content: center;
  align-items: center;
  color: var(--text-color);
  opacity: 0.6;
}

.empty-icon {
  font-size: clamp(1.5rem, 3vw, 2rem);
  margin-bottom: 10px;
  opacity: 0.7;
}

.game-hint {
  margin-top: 10px;
  font-size: 0.8rem;
  opacity: 0.7;
  color: var(--accent-color);
}

/* æ¸¸æˆå®¹å™¨ */
.game-container {
  width: 100%;
  height: 100%;
  display: flex;
  flex-direction: column;
  overflow: hidden;
  padding: 0;
  margin: 0;
}

/* å½•éŸ³åŠ¨ç”» */
@keyframes pulse {
  0% {
    box-shadow: 0 0 0 0 rgba(231, 76, 60, 0.4);
  }
  70% {
    box-shadow: 0 0 0 10px rgba(231, 76, 60, 0);
  }
  100% {
    box-shadow: 0 0 0 0 rgba(231, 76, 60, 0);
  }
}

/* ä¸»é¢˜é€‚é… */
:deep(.theme-light) .notes-input,
:deep(.theme-github) .notes-input {
  background-color: rgba(255, 255, 255, 0.8);
  border: 1px solid rgba(0, 0, 0, 0.1);
  color: #000000;
}

:deep(.theme-light) .notes-input::placeholder,
:deep(.theme-github) .notes-input::placeholder {
  color: #666;
}

:deep(.theme-light) .note-item,
:deep(.theme-github) .note-item {
  background-color: rgba(0, 0, 0, 0.03);
  color: #000000;
}

:deep(.theme-light) .note-content,
:deep(.theme-github) .note-content {
  color: #000000;
}

:deep(.theme-light) .note-footer,
:deep(.theme-github) .note-footer {
  color: #000000;
}

:deep(.theme-light) .notes-empty,
:deep(.theme-github) .notes-empty {
  color: #000000;
}

:deep(.theme-light) .speech-btn,
:deep(.theme-github) .speech-btn {
  background-color: rgba(0, 0, 0, 0.1);
  color: #000000;
}

:deep(.theme-light) .speech-status,
:deep(.theme-github) .speech-status {
  color: #000000;
}

/* å“åº”å¼è°ƒæ•´ */
@media (max-width: 768px) {
  .notes-widget {
    padding: 5px;
    min-height: 150px;
  }
  
  .notes-input {
    height: 50px;
  }
}

@media (orientation: portrait) {
  .notes-widget {
    min-height: 300px;
  }
  
  .notes-input {
    height: clamp(50px, 10vh, 70px);
  }
  
  .notes-list {
    min-height: 100px;
  }
}

/* æ¨ªå±æ¨¡å¼ç‰¹åˆ«å¤„ç† */
@media (orientation: landscape) {
  .notes-widget {
    min-height: 250px;
    max-height: 100%;
  }
  
  .notes-list {
    overflow-y: auto;
  }
}

.speech-btn.disabled {
  opacity: 0.5;
  cursor: not-allowed;
}

.speech-error {
  font-size: 12px;
  color: #e74c3c;
  margin-right: 10px;
}

/* è¯­éŸ³è¯†åˆ«ä¸å¯ç”¨æ¨¡æ€æ¡† */
.speech-unavailable-modal {
  position: fixed;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
  background-color: rgba(0, 0, 0, 0.5);
  display: flex;
  justify-content: center;
  align-items: center;
  z-index: 1000;
}

.modal-content {
  background-color: var(--widget-bg-color);
  border-radius: 8px;
  padding: 20px;
  width: 80%;
  max-width: 400px;
  box-shadow: 0 4px 8px rgba(0, 0, 0, 0.2);
}

.modal-content h3 {
  margin-top: 0;
  color: var(--text-color);
  border-bottom: 1px solid var(--accent-color);
  padding-bottom: 10px;
}

.modal-content p {
  margin: 15px 0;
  color: var(--text-color);
  line-height: 1.5;
}

.modal-actions {
  display: flex;
  justify-content: flex-end;
  margin-top: 20px;
}

.modal-actions button {
  padding: 8px 15px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
}

.recording-actions {
  display: flex;
  justify-content: space-between;
  margin: 20px 0;
}

.recording-actions button {
  padding: 8px 15px;
  border: none;
  border-radius: 4px;
  cursor: pointer;
  display: flex;
  align-items: center;
  justify-content: center;
}

.recording-actions button span {
  margin-right: 5px;
}

.secondary-btn {
  background-color: rgba(var(--accent-rgb), 0.2);
  color: var(--accent-color);
}

.secondary-btn:hover {
  background-color: rgba(var(--accent-rgb), 0.3);
}
</style>

<script lang="ts">
// ä¸ºTypeScriptæ·»åŠ SpeechRecognitionç±»å‹å£°æ˜
declare global {
  interface Window {
    SpeechRecognition: typeof SpeechRecognition;
    webkitSpeechRecognition: typeof SpeechRecognition;
    electronAPI?: {
      sendReady: () => void;
      requestGC: () => void;
      setLowMemoryMode: (enabled: boolean) => void;
      onMemoryPressure: (callback: (data: any) => void) => void;
      onVisibilityChange: (callback: (state: string) => void) => void;
      getSystemInfo: () => any;
      requestMicrophonePermission: () => Promise<{granted: boolean, status: string}>;
      checkMicrophonePermission: () => Promise<{status: string}>;
      testSpeechRecognition: () => Promise<{available: boolean, reason?: string, message?: string}>;
      logDebug: (message: string) => void;
    };
    process?: {
      type: string;
    };
  }
}
</script> 